/*
-- Query: SELECT * FROM `whit-e.com`.postings
LIMIT 0, 1000

-- Date: 2019-10-26 11:43
*/
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Build a Facebook Messenger Bot (Node.js)','Dev, Node.JS','You would like to have your own messenger bot but are still looking for the right entry level?\nHere I will explain step by step how to create your own messenger bot, what to consider and how to interact with a user.','\nMessenger bots uses a web server to process messages it receives or to figure out what messages to send. You also need to have the bot be authenticated to speak with the web server and the bot approved by Facebook to speak with the public.\n\nIf you want to see a working messenger bot, jump to the end of the article there and I\'ll show you one of my bots.\n\n### Prerequisites\n\nOur messenger bot is written in [Node.js]https://nodejs.org/en/), so it is important (for the local environment) to install Node.js and recommend to use an IDE with syntax highlighting for Javascript (e. g. using  [Sublime](https://www.sublimetext.com/))\n\n### Step 1 - Install Heroku-CLI\n\nThis guide refers to how to run the Messenger bot on a Heroku server. Therefore it is important that we first  [register](https://signup.heroku.com/)  at Heroku and then install their CLI - https://devcenter.heroku.com/articles/heroku-cli.\n\nWhy the instructions refer to a Heroku server? Now quite simply, Facebook expects an https connection for every messenger bot to make sure that the connection is encrypted. Heroku offers us exactly that, and it is also possible to run several WebApps for free.\n\n### Step 2 - Create Node.js Project\n\nNext we have to create a new Node. js project for our bot. To do this, we create a folder somewhere and enter the following command there:\n\n```\nnpm init\n\n\n```\n\n### Step 3 - Additional Dependencies\n\nNow we install the additional Node dependencies. express = is needed for the server request = is needed for sending out messages body-parser = is needed to process messages.\n\n```\nnpm install express request body-parser --save\n\n\n```\n\n### Step 4 - Create the Index.js\n\nNow we create with the most important file of our bot, the index.js. We copy the following content into it\n\n```javascript\n     \'use strict\'\n     \n     const express = require(\'express\')\n     const bodyParser = require(\'body-parser\')\n     const request = require(\'request\')\n     const app = express()\n \n     app.set(\'port\', (process.env.PORT || 5000))\n \n     // Process application/x-www-form-urlencoded\n     app.use(bodyParser.urlencoded({extended: false}))\n \n     // Process application/json\n     app.use(bodyParser.json())\n \n     // Index route\n     app.get(\'/\', function (req, res) {\n     	res.send(\'Hello world, I am a chat bot\')\n     })\n \n     // for Facebook verification\n     app.get(\'/webhook/\', function (req, res) {\n     	if (req.query[\'hub.verify_token\'] === \'very_awesome_verify_token\') {\n     		res.send(req.query[\'hub.challenge\'])\n     	}\n     	res.send(\'Error, wrong token\')\n     })\n \n     // Spin up the server\n     app.listen(app.get(\'port\'), function() {\n     	console.log(\'running on port\', app.get(\'port\'))\n     })\n\n\n```\n\n### Step 5 - Commit & Push to Heroku\n\nFirst we make a file called Procfile and copy this. We need this file so that Heroku knows the starting point of our bot. File Content:\n\n```\nweb: node index.js\n\n\n```\n\nAfter that, we commit all the code with Git and create a new Heroku instance, then we push the code to the cloud.\n\n```\ngit init\ngit add .\ngit commit --message \"hello world\"\nheroku create\ngit push heroku master\n\n\n```\n\n### Step 6 - Setup the Facebook App\n\n1.  Create or configure a new Facebook App or Page here https://developers.facebook.com/apps/\n    \n2.  In the app go to Messenger tab then click Setup Webhook. Here you will put in the URL of your Heroku server and a token. Make sure to check all the subscription fields.\n    \n3.  Get your Page Access Token and save this.\n    \n4.  Go back to Terminal and type in this command to trigger the Facebook app to send messages. Remember to use the token you requested earlier.\n    \n    ```bash\n    curl -X POST \"https://graph.facebook.com/v2.6/me/subscribed_apps?access_token=<PAGE_ACCESS_TOKEN>\"\n    \n    \n    ```\n    \n\n### Step 7 - Setup the bot\n\nIn order for Facebook and Heroku to know each other, we need to make some adjustments to our bot.\n\n1.  We have to add an API endpoint to index.js for processing messages and include our token, which we got earlier.\n    \n    ```javascript\n    app.post(\'/webhook/\', function (req, res) {\n        let messaging_events = req.body.entry[0].messaging\n        for (let i = 0; i < messaging_events.length; i++) {\n    	    let event = req.body.entry[0].messaging[i]\n    	    let sender = event.sender.id\n    	    if (event.message && event.message.text) {\n    		    let text = event.message.text\n    		    sendTextMessage(sender, \"Text received, echo: \" + text.substring(0, 200))\n    	    }\n        }\n        res.sendStatus(200)\n    })\n    \n    const token = \"<PAGE_ACCESS_TOKEN>\"\n    \n    \n    ```\n    \n    **Attention!!**: Please keep your app secrets out of the version control! I have already seen several projects on Github that shared private_keys, app_secrets etc. publicly.\n    \n2.  Add a function to echo back messages\n    \n    ```javascript\n    function sendTextMessage(sender, text) {\n        let messageData = { text:text }\n        request({\n    	    url: \'https://graph.facebook.com/v2.6/me/messages\',\n    	    qs: {access_token:token},\n    	    method: \'POST\',\n    		json: {\n    		    recipient: {id:sender},\n    			message: messageData,\n    		}\n    	}, function(error, response, body) {\n    		if (error) {\n    		    console.log(\'Error sending messages: \', error)\n    		} else if (response.body.error) {\n    		    console.log(\'Error: \', response.body.error)\n    	    }\n        })\n    }\n    \n    \n    ```\n    \n3.  Commit the code again and push to Heroku\n    \n    ```\n    git add .\n    git commit -m \'updated the bot to speak\'\n    git push heroku master\n    \n    \n    ```\n    \n4.  Go to the Facebook Page and click on Message to start chatting!\n    \n\n## Optional: How to share my Bot?\n\nTo share your Facebook Bot with others you have 2 options:\n\n### _Add a chat Button_\n\nGo  [here](https://developers.facebook.com/docs/messenger-platform/plugin-reference)  to learn how to add a chat button your page.\n\n### _Create a shortlink_\n\nYou can use https://m.me/<PAGE_USERNAME> to have someone start a chat.\n\n## My Messenger Bot \"searchify\"\n\nSearchify is a Facebook Messenger bot that sends your search terms 1:1 to Google and returns the top search results back to you.\n\nThe code is open source and anyone can use it as the basis for a new bot. I have removed all KEYs and TOKEN.\n\nThis Messenger Bot is hosted on Heroku ->  [https://search1fy.herokuapp.com](https://search1fy.herokuapp.com/)','2018-06-10 00:00:00',0,'simple-messenger-bot');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Build a Game - 2048 (Java)','Dev, Java','Currently only the game sources are available. I developed it in 2014. If I have enough time, I will refactor it and write a HOW TO for it. \nUntil then you can view and download it on Github! ','Currently only the game sources are available. I developed it in 2014. If I have enough time, I will refactor it and write a HOW TO for it. Until then you can view and download it on Github!\n\n[https://github.com/whit-e/2048](https://github.com/whit-e/2048)','2014-04-24 00:00:00',0,'2048-simple-java-game');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Build a simple WebCrawler (Python)','Dev, Python','You want to crawl a website and index the data to a search like ElasticSearch, but don\'t know how? Have a look here - I hope I can help you. I was recently confronted with the same task and found a good solution for this problem.','# Build a simple WebCrawler (Python)\n\n### Prerequisites\n\nTo complete this tutorial, you\'ll need a local development environment for Python 3. You can follow this Tutorial: \"[How To Install and Set Up a Local Programming Environment for Python 3](https://www.digitalocean.com/community/tutorial_series/how-to-install-and-set-up-a-local-programming-environment-for-python-3)\" to configure everything you need.\n\n[https://github.com/whit-e/2048](https://github.com/whit-e/2048)\n\n### Step 1 - Install Dependencies\n\nBefore we create a new crawler, we must first install the required dependencies. These are the following:\n\n-   **Scrapy**  this is our later crawler`pip install scrapy`\n-   **BeautifulSoup4**  we need this for parsing the crawled content better`pip install beautifulsoup4`\n-   __**Optional**_:  _**ScrapyElasticSearch**__  we only need this if we want to connect the crawler with an ElasticSearch Index`pip install ScrapyElasticSearch`\n\n### Step 2 - Create a new Spider\n\nTo create a new crawler / spider we first need a new scrapy project (the most practical thing about scrapy is that it takes a lot of work for us).`scrapy startproject wiki testcrawler`Next, we switch to the new folder and generate our crawler:`cd testcrawler``scrapy genspider wiki wikipedia.org`Now we have created the crawler\"  **/testcrawler/wiki/spiders/wikipedia. py**\".\n\nIf you open this file, you can see the basic structure of a crawler:\n\n```\n # -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass WikipediaSpider(scrapy.Spider):\n    name = \'wikipedia\'\n    allowed_domains = [\'wikipedia.org\']\n    start_urls = [\'http://wikipedia.org/\']\n\n    def parse(self, response):\n        pass\n\n```\n\n-   _name_  = the name of the crawler\n-   _allowed_domains_  = an array with the allowed domains\n-   _start_urls_  = an array with the starting points for the crawler\n\nTheoretically, we can already run it now. However, since no items have been defined yet, it will not give us any data at the moment.`scrapy crawl wikipedia`\n\n**Optional**  _You can enter the parameter \"-o\" and then enter a filename where you want the crawled results to be stored. This looks like the following:_`scrapy crawl wikipedia -o test.json`\n\n### Step 3\n\nBefore we can retrieve data from the site by using our crawler, we first have to define which data we want specifically. To do this, we go to the  **/testcrawler/wiki/items.py**  which has already been generated and add our fields to the WikiItem, which will look like this:\n\n```\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass WikiItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = scrapy.Field()\n    content = scrapy.Field()\n\n```\n\n### Step 4\n\nNext, we make sure that our crawler finds the right content. First we create the method \"***createItem()***\". This gets our response later. There we create an item of the type \"_**WikiItem**_\" which we have to import before. Afterwards we create a BeautifulSoup Object and give it the crawled content. I recommend BeautifulSoup because it takes some work off our hands.\n\nTo assign the corresponding content to our WikiItem now, we simply say  _**item[\' title\'] = soup.h1. text**_  This will take the first found H1 from Soup and displays only the text. We get this later as a title for our crawled element.\n\nFinally we only have to iterate in the \"parse\"-method about all contents which have the CSS-ID  _**\"#content\"**_  and then pass their data to our method \"createItem\". The code for this looks like this:\n\n**/testcrawler/wiki/spiders/wikipedia.py**\n\n```\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom bs4 import BeautifulSoup\nfrom wiki.items import WikiItem\n\nclass WikipediaSpider(scrapy.Spider):\n    name = \'wikipedia\'\n    allowed_domains = [\'wikipedia.org\']\n    start_urls = [\'https://de.wikipedia.org/wiki/Webcrawler\']\n\n    def createItem(self, response):\n        item = WikiItem()\n        soup = BeautifulSoup(response.extract(), \'html.parser\')\n        item[\'title\'] = soup.h1.text\n        item[\'content\'] = soup.get_text()\n        return item\n\n    def parse(self, response):\n        responseSelector = scrapy.Selector(response)\n        for sectionId, section in enumerate(responseSelector.css(\'#content\')):\n            yield self.createItem(section)\n\n```\n\n**/testcrawler/wiki/items.py**\n\n```\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass WikiItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = scrapy.Field()\n    content = scrapy.Field()\n\n```\n\nIf we now execute the crawler with \"_**scrapy crawl wikipedia -o test.json**_\" we get the content of the crawled page as Json File. That\'s all it takes to build a simple crawler.\n\n### Optional: Put Data into an ElasticSearchIndex\n\nTo index our crawled page in our ElasticSearch search we just have to install the plugin \"_**ScrapyElasticSearch**_\".`pip install ScrapyElasticSearch`And add the following settings at the end of the file in the \"  _**/testcrawler/wiki/settings.py**_\" :\n\n```\nITEM_PIPELINES = {\n    \'scrapyelasticsearch.scrapyelasticsearch.ElasticSearchPipeline\': 500\n}\nELASTICSEARCH_SERVERS = [\'localhost:9200\']\nELASTICSEARCH_INDEX = \'wikipedia_search\'\nELASTICSEARCH_TYPE = \'wiki\'\nELASTICSEARCH_UNIQ_KEY = \'title\'\n\n```\n\nIf our ElasticSearchServer is now running and can be reached under the following port, Scrapy will automatically index the results (like shown in our test.json). If you specify such a pipeline, you should make sure that the server is always up and running. Otherwise you should implement an exception handler for the connection timeout to your ElasticSearch Server.\n\nAlternatively, you can also include a query in your crawler that uses the environment variables and then set the settings for the pipeline directly in your crawler. To do this, you have to import \"os\" at the beginning of the crawler, so we can access the environment variables. The whole thing could look like this:\n\n```\nimport os\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import Spider, Rule\nfrom scrapy.selector import Selector\nfrom bs4 import BeautifulSoup\nfrom wiki.items import WikiItem\n\nclass WikipediaSpider(Spider):\n    name = \"wikipedia\"\n    if \"ELASTICSEARCH_IP\" in os.environ and \"ELASTICSEARCH_INDEX\" in os.environ:\n        custom_settings = {\n            \'ITEM_PIPELINES\': {\'scrapyelasticsearch.scrapyelasticsearch.ElasticSearchPipeline\': 500},\n            \'ELASTICSEARCH_INDEX\': os.environ[\'ELASTICSEARCH_INDEX\'],\n            \'ELASTICSEARCH_TYPE\': \'ourItemType\',\n            \'ELASTICSEARCH_UNIQ_KEY\': \'uid\',\n            \'ELASTICSEARCH_SERVERS\': [os.environ[\'ELASTICSEARCH_IP\']]\n        }\n\n```\n\n### Code Example\n\nOn my  [Github](https://github.com/whit-e/testcrawler)  you will find the example project below - without the ElasticSearch connection.\n\nHere is a small project that crawls a certain number of Wikipedia pages into an elastic index. In addition, there is a web page for querying the elastic index. Project:  [https://github.com/whit-e/wiki-crawler](https://github.com/whit-e/wiki-crawler)  ElasticDemoPage:  [http://whit-e.com/wiki-crawler-example](http://whit-e.com/wiki-crawler-example)  - not 100% responsive.','2018-05-03 00:00:00',0,'simple-webcrawler');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Cloud Application #1 A) Build your own Node.js Microservice','Dev, Node.JS, Cloud Application','In the tutorial series \"Cloud Application\" I show you how to build and deploy a MicroService into the Google Cloud. With all buzzwords like \"cloud\", \"api\", \"microservice\", \"docker\", \"ci/cd\" & so on!','\n## Prerequisites\n- installed Node.js and Npm \n- basic knowledge of JavaScript and Node.js\n- enjoy programming\n\n## Preparations\nIn this part we will create our base project, remove unused files and install all necessary dependencies which we will work with later.\nFirst of all we need the express generator, which gives us a basic structure for our application:\n``npm install express-generator -g``. Now we can build our application: ``express --no-view yourApplicationName``\nNow the following structure should have been created:\n```\n   yourApplicationName\\\n    |-- \\public\\\n    |  |-- \\javascripts\\\n    |  |-- \\images\\\n    |  |-- \\stylesheets\\\n    |  |-- |--\\style.css\n    |--|-- \\index.html\n    |-- \\routes\\\n    |  |-- \\index.js\n    |  |-- \\users.js\n    |-- \\app.js\n    |-- \\package.json\n    |-- \\bin\\www                     \n```\nFrom the automatically generated files we can delete the folders public and routes, cause we don´t need them.\nNext we need to install 2 packages which we need for connecting to the database (mysql) through our application:\n```python\n#switch to your application folder\ncd yourApplicationName\n#installs the missing dependencies, which are already declared at package.json\nnpm install \nnpm install mysql\n#only needed if you have a timestamp field\nnpm install momentum\n```\n	*What you can optionally install is **nodemon**: **\"Monitor for any changes in your node.js application and automatically restart the server - perfect for development\"** you could start your Application with nodemon app.js instead of npm start*\nNow we have prepared everything so far to start with the implementation of our service.\n\n## Build our Service\nFirst we open, with the IDE of yourchoice, the ``app.js`` and remove there the lines 6,7,15, 17 and 18, cause our application could not start with these (we already removed the files for this lines of code).\nNow our ``app.js``  should look like this:\n```javascript\nvar express  = require(\'express\');\nvar path = require(\'path\');\nvar cookieParser =  require(\'cookie-parser\');\nvar logger =  require(\'morgan\');  \n\nvar app = express();\n\napp.use(logger(\'dev\'));\napp.use(express.json());\napp.use(express.urlencoded({ extended:  false }));\napp.use(cookieParser());\n\nmodule.exports = app;\n```\nTo validate that our application is running, we can insert a simple ``console.log(\"Hello World\");``  in the line before ``module.exports = app;``.\nNow we can start our application for the first time, either with ``npm start``  or with ``nodemon ./bin/www`` (if it is installed).\n*I prefer nodemon because this way we start the application only once and everytime when we edit our code, the changes will be applied automatically.* #**hotreloading**\nIf everything works well, we now receive \"Hello World\".\nFine.\nAs a next step we want to display \"Hello World\" at our Browser.\nFor that, we want to implement an Express Route, which listen on the path \"/\", use an HTTP-GET method and sends us as response \"Hello World\".\n\n```javascript\napp.get(\'/\', (request, response) => {\n	response.send(\"Hello World\");\n});\n```\nIf you know open your Browser and type \"localhost:3000/\" you should see there \"Hello World\". \n*Info: Instead of the GET method ( ``app.get(\'/\', (req, res) => {});`` you can also use all other HTTP methods ( app.post, app.put, app.delete)*\n\nNow we need 3 routes for our Microservice -> ``insert`` ``getAll`` ``getByName``. I have decided that all 3 will be a normal GET-Method. The reson for that is, you can simply call them using your browser and you don\'t have to install any other programs (like curl, postman etc.). *For \"insert\" POST would also be a good choice.* \n\n```javascript\napp.get(\'/insert\', (request, response) => {\n	response.send(\"insert something to our mysql table\");\n});\napp.get(\'/getByName/:name\', (request, response) => {\n	response.send(\"show only entry for: \" + request.params.name);\n});\napp.get(\'/getAll\', (request, response) => {\n	response.send(\"show all entries\");\n});\n/** optional method to check if our microservice is running\napp.get(\'/health\', (request, response) => {\n	response.send(\"everything is fine\");\n});\n**/\n```\nYou will now notice a special detail in the routes. \"*getByName*\" still has the parameter :name by this syntax we can pass parameters directly to the path and the route will only take effect if ``/getByName/`` has a value afterwards. This way we save a Null check. \nWe can access the URL parameters as follows:\n```javascript\nrequest.params.yourParam 	->	defined in the route ( /:yourParam/:yourParam2\nrequest.query.yourQuery		-> 	defined in the url	 ( /get?yourQuery=1234&yourQuery2=1)\nrequest.body.yourParam		-> 	is often used by POST-Methods\n```\nUp to this point we already have a microservice which provides us with mocked answers and integrated hotreloading. The last step is to access the data from the database.\n\n## Connect to the Database\nIn addition to routing to path XYZ, we also need a database from which we load / save the corresponding data. For this I simply used a MySQL (at remotemysql.com there are e.g. free databases).\nIn our Preparations we have already installed the Momentum and Mysql dependencies for nodejs. Info: *Momentum is only needed to create a valid timestamp (so if you don\'t need a timestamp - you don\'t have to install it).*\nFirst we import our packages:\n``` javascript\nvar mysql = require(\'mysql\');\nvar momentum = require(\'momentum\');\n```\nThe next step is to create a global variable for the connection below the import section:\n```\nvar connection = mysql.createConnection({\n	host: \"localhost\",\n	// never save credentials to your source code, store them in environment variables!\n	user: process.env.dbUser,\n	password: process.env.dbPassword, \n	database: \"examples\"\n}\n```\nIf your database is running and you can access them with your user without any problems, everything should works! Next we will implement the ``getAll`` method. For that, we will replace the existing code with the following one: \n```javascript\n// http://localhost:3000/getAll\napp.get(\'/getAll\', (request, response) => {\n	connection.query(\"SELECT * FROM someTable\", function (err, result, fields) {\n		if (err) return response.send(err);\n		return response.json(result);\n	});\n});\n```\nWe now send the query \"``SELECT * FROM someTable``\" from our microservice to our database, should any SQL errors occur because e.g. the table / database does not exist or something else - we send the error als response otherwise, If everything works, we send a json with the data as answer.\n\nNext, we implement the ``/getByName/:name`` Method:\n```javascript\n// http://localhost:3000/getByName/data\napp.get(\'/getByName/:name\', (req, res) => {\n	con.query(\"SELECT * FROM someTable WHERE name = \" + mysql.escape(req.params.name), function (err, result, fields) {\n		if (err) return res.send(err);\n		return res.json(result);\n	});\n});\n```\nInfo: *When query values are variables provided by the user, you should escape the values, like that \"mysql.escape( )\" otherwise you risk a sql injection.*\nFinally we create our insert function:\n```javascript\napp.get(\'/insert\', (req, res) => {\n	if(typeof req.query.name !== \'undefined\' && typeof req.query.text !== \'undefined\' && typeof req.query.description !== \'undefined\') {\n		var sql = \"INSERT INTO someTable (uid, timestamp, name, text, description ) VALUES ?\";\n		var values = [[uuidv4(), moment().utc().format(\"YYYY-MM-DD\"), req.query.name, req.query.text, req.query.description ]]\n		con.query(sql, [values] ,function (err, result) {\n			if (err) return res.send(err);\n			return res.send(\"insert successfully: \" + result);\n		});\n	}\n});\n\n// generates a uuid\nfunction uuidv4() {\n	return \'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx\'.replace(/[xy]/g, function(c) {\n		var r = Math.random() * 16 | 0, v = c == \'x\' ? r : (r & 0x3 | 0x8);\n		return v.toString(16);\n	});\n}\n```\nHere we first ask if all query parameters are existing. Then we create our SQL statement and pass the required values to con.query as second parameter.\n\nThat would be it. Now we have a running MicroService, which retrieves data from a MySQL and provides them as JSON via the endpoints.\nYour app.js should now look like this:\n```javascript\nvar express = require(\'express\');\nvar path = require(\'path\');\nvar cookieParser = require(\'cookie-parser\');\nvar logger = require(\'morgan\');\nvar mysql = require(\'mysql\');\nvar moment = require(\'moment\');\n\nvar app = express();\n\napp.use(logger(\'dev\'));\napp.use(express.json());\napp.use(express.urlencoded({ extended: false }));\napp.use(cookieParser());\n\nvar con = mysql.createConnection({\n	host: \"localhost\",\n	user: process.env.dbUser,\n	password: process.env.dbPassword,\n	database: \"examples\"\n});\n\n// http://localhost:3000/insert?name=someText&text=irgendein&description=lol\napp.get(\'/insert\', (req, res) => {\n	if(typeof req.query.name !== \'undefined\' && typeof req.query.text !== \'undefined\' && typeof req.query.description !== \'undefined\') {\n		var sql = \"INSERT INTO someTable (uid, timestamp, name, text, description ) VALUES ?\";\n		var values = [[uuidv4(), moment().utc().format(\"YYYY-MM-DD\"), req.query.name, req.query.text, req.query.description ]]\n		con.query(sql, [values] ,function (err, result) {\n			if (err) return res.send(err);\n			return res.send(\"insert successfully: \" + result);\n		});\n	}\n});\n\n// http://localhost:3000/getByName/data\napp.get(\'/getByName/:name\', (req, res) => {\n	con.query(\"SELECT * FROM someTable WHERE name = \" + mysql.escape(req.params.name), function (err, result, fields) {\n		if (err) return res.send(err);\n		return res.json(result);\n	});\n});\n\n// http://localhost:3000/getAll\napp.get(\'/getAll\', (req, res) => {\n	con.query(\"SELECT * FROM someTable\", function (err, result, fields) {\n		if (err) return res.send(err);\n		return res.json(result);\n	});\n});\n\n// generates a uuid\nfunction uuidv4() {\n	return \'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx\'.replace(/[xy]/g, function(c) {\n		var r = Math.random() * 16 | 0, v = c == \'x\' ? r : (r & 0x3 | 0x8);\n		return v.toString(16);\n	});\n}\n\nmodule.exports = app;\n```\n\n\n### Next Step:   [Dockerizing our Service](https://whit-e.com/articles/dockerizing-our-service)\n\n***Info**/ **best practice**: Why should I always check the database for each request? Just create a **cache**. When the server starts or adds data to the database, it is filled with data. Advantage: significantly less load on the system and your database*\n\n#### Project: [GitHub: microservice_blueprint_nodejs](https://github.com/whit-e/microservice_blueprint_nodejs)','2019-10-13 00:00:00',0,'building-nodejs-rest-api');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Cloud Application #1 B) Build your own Java Microservice','Dev, Java, Cloud Application','In the tutorial series \"Cloud Application\" I show you how to build and deploy a MicroService into the Google Cloud. With all buzzwords like \"cloud\", \"api\", \"microservice\", \"docker\", \"ci/cd\" & so on!','\n## Prerequisites\n- installed java 8\n- installed gradle\n- basic knowledge of java and gradle\n- ide with gradle support (i use eclipse)\n- enjoy programming\n\n## Preparations\nIn this part we will create our base project, remove unused files and install all necessary dependencies which we will work with later.\nFirst of all we open our IDE (this tutorial is based on eclipse only) and create a ``new`` > `` Gradle Project``.\nIn the Gradle Setup we give our project a name on the first step (in my case again ``yourApplicationName``). In the next step we leave all settings as they are and click on finish.\n\nNow the following structure should have been created:\n```\n   yourApplicationName\\\n    |-- \\src\\main\\java\n    |  |-- \\(default package)\\\n    |  |-- |--\\Library.java\n    |-- \\src\\test\\java\n    |  |-- \\(default package)\\\n    |  |-- |--\\LibraryTest.java\n    |--|-- \\index.html\n    |-- \\gradle\\wrapper\n    |  |-- \\gradle-wrapper.jar\n    |  |-- \\gradle-wrapper.properties\n    |-- \\build.gradle\n    |-- \\gradlew\n    |-- \\gradlew.bat\n    |-- \\settings.gradle                     \n```\nFrom the automatically generated files we can delete the Files ``Library.java`` and ``LibraryTest.java``, cause we don´t need them.\nNext we add all necessary dependencies to our ``build.gradle``:\n```gradle\ndependencies {\n// Use JUnit test framework\ntestImplementation \'junit:junit:4.12\'\n\n// Jetty\ncompile group: \'org.eclipse.jetty\', name: \'jetty-server\', version: \'9.4.12.v20180830\'\ncompile group: \'org.eclipse.jetty\', name: \'jetty-servlet\', version: \'9.4.12.v20180830\'\n\n// Gson - needed for Json\ncompile group: \'com.google.code.gson\', name: \'gson\', version: \'2.8.5\'\n\n// Java API for RESTful Web Services\ncompile group: \'javax.ws.rs\', name: \'javax.ws.rs-api\', version: \'2.1.1\'\n\n// Glassfish\ncompile group: \'org.glassfish.jersey.containers\', name: \'jersey-container-servlet-core\', version: \'2.27\'\ncompile group: \'org.glassfish.jersey.inject\', name: \'jersey-hk2\', version: \'2.27\'\n\n// MySQL Connector\ncompile group: \'mysql\', name: \'mysql-connector-java\', version: \'8.0.13\'\n}\n```\nAfter updating our gradle project ``right click on the project``>``Gradle``<``Refresh Gradle Project``, everything is now ready to implement the service\n\n## Build our Service\nFirst we create the class SampleDBObj with the following content:\n```java\nimport java.util.Date;\nimport java.util.UUID;\n\npublic class SampleDBObj\n{\n    private Date timestamp;\n    private String uid;\n    private String name;\n    private String text;\n    private String description;\n\n    public SampleDBObj(String name, String text, String description) {\n    	this.timestamp = new Date();\n    	this.uid = UUID.randomUUID().toString();\n    	this.name = name;\n    	this.text = text;\n    	this.description = description;\n\n    }\n    \n    public SampleDBObj(Date timestamp, String uid, String name, String text, String description) {\n    	this.timestamp = timestamp;\n    	this.uid = uid;\n    	this.name = name;\n    	this.text = text;\n    	this.description = description;\n\n    }\n\n	public String toCSVLine() {\n		return this.timestamp + \",\" + this.uid + \",\" + this.name + \",\" + this.text + \",\" + this.description;\n	}\n\n	public Date getTimestamp() {\n		return timestamp;\n	}\n\n	public String getUid() {\n		return uid;\n	}\n\n	public String getName() {\n		return name;\n	}\n\n	public String getText() {\n		return text;\n	}\n\n	public String getDescription() {\n		return description;\n	}\n	\n}\n```\nThis class only represents the objects that we will use later for our service.\nNext we create our Servlet class. \nThis class receives the requests within the web server and processes them. If you don\'t know exactly what a servlet is, you can find a very good description at Wikipedia: https://en.wikipedia.org/wiki/Java_servlet \nWith a Servlet our routes are specified like e.g. \"/getAll\" and it is said with which HTTP method this is called. Then business logic happens and the client receives a response from the web server.\nBut now lets create our Servlet-Class, called \"SampleServlet.java\":\n```java\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.QueryParam;\nimport javax.ws.rs.core.MediaType;\n\n@Path(\"/\")\npublic class SampleServlet {\n	\n\n	// http://localhost:3001/api/v1/someMethod?someParam=name\n	@Path(\"helloWorld\")\n	@GET //HTTP Method\n	@Produces(MediaType.APPLICATION_JSON) // Response Type of the Route (with a REST-API often used e.g. json) \n	public String helloWorld(@QueryParam(\"someParam\") String name) {\n		return \"My name is: \" + name;			\n	}	\n}\n\n```\nHere you can see, our servlet is very simple. It can be reached directly under \"/\". If we look at the method ``helloWorld`` we see some annotations above the method declaration.\nThese are used to say under which path (``@Path(\"pathForTheMethod\")``) in the servlet this method is available, which HTTP method (``@GET`` for GET-Method) must be used for the request and which MediaType (`` @Produces(MediaType.APPLICATION_JSON)``) produces the response. \nWe also pass a parameter to our method. Through the annotation ``@QueryParam`` our service later knows - this is a query parameter which is passed through the URL.\n\nEverything was still missing to have a really rudimentary MicroService is the Main Class. Our server, where also the servlets are registered. For this we create the class \"Service.java\". Our service will look like this:\n```java\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.servlet.ServletContextHandler;\nimport org.eclipse.jetty.servlet.ServletHolder;\nimport org.glassfish.jersey.servlet.ServletContainer;\n\npublic class Service {\n\n	public static void main(String[] args) {\n		new Service().startup();\n	}\n\n	public boolean startup() {\n		ServletContextHandler context = new ServletContextHandler(ServletContextHandler.NO_SESSIONS);\n		context.setContextPath(\"/\");\n		\n		Server server = new Server(3001);\n		server.setHandler(context);\n		\n		ServletHolder servlet = context.addServlet(ServletContainer.class, \"/service/*\");\n		servlet.setInitParameter(\"jersey.config.server.provider.classnames\", SampleServlet.class.getCanonicalName());\n		\n		try {\n			server.start();\n			return true;\n		} catch (Exception e) {\n			// TODO Auto-generated catch block\n			e.printStackTrace();\n		}\n		return false;\n	}\n}\n```\nHere we first create a ServletContextHandler, which we need to register our Servlet there. \nNext we tell the ContextHandler to listen to everything under the root path (``setContextPath(\"/\");``). Then we create the web-server with the port and tell the server which handler it should use. After that, we register our service under the path \"service\".\nIf we now start our service and call the page http://localhost:3001/service/helloWorld?someParam=Hello+World we should get the following result:\n``My name is: Hello World``\nIf you get that response, everything works! Very good!\nNext we extend our servlet with the required methods: ``insert``, ``getByName``, ``getAll`` ``export``. \nBecause we implement it in the same way as our Hello World method, I skip the code at this point.\n\n## Versioned API\n**Info**: *If you don\'t have to use a versioned API and want to use only one servlet for your service - you could skip this point*\n\nNow we optimize our service a little bit.\nWhy? I always prefer a versioned API. Our Service now looks like that:\n```java\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.servlet.ServletContextHandler;\nimport org.eclipse.jetty.servlet.ServletHolder;\nimport org.glassfish.jersey.servlet.ServletContainer;\n\npublic class Service {\n\n	public static void main(String[] args) {\n		\n		new Service().startup();\n	}\n\n	public boolean startup() {\n		ServletContextHandler context = new ServletContextHandler(ServletContextHandler.NO_SESSIONS);\n		context.setContextPath(\"/\");\n		\n		Server server = new Server(3001);\n		server.setHandler(context);\n		\n		registerServletsV1(context);\n		\n		try {\n			server.start();\n			return true;\n		} catch (Exception e) {\n			// TODO Auto-generated catch block\n			e.printStackTrace();\n		}\n		return false;\n	}\n	\n	private void registerServletsV1(ServletContextHandler context) {\n		Map<String, Class<?>> mappings = new HashMap<>();\n		// name where our servlet is available\n		mappings.put(\"service\", SampleServlet.class);\n		// api version\n		registerServlets(context, \"v1\", mappings);\n	}\n\n	private void registerServlets(ServletContextHandler context, String version,\n		Map<String, Class<?>> mappings) {\n\n		for (String key : mappings.keySet()) {\n			Class<?> value = mappings.get(key);\n\n			// /api/v1/service/getAll for example\n			ServletHolder servlet = context.addServlet(ServletContainer.class, \"/api/\" + version + \"/\" + key + \"/*\");\n			servlet.setInitParameter(\"jersey.config.server.provider.classnames\", value.getCanonicalName());\n		}\n	}\n	\n}\n```\nWhat\'s new? The ``registerServletsV1`` method and the ``registerServlets`` method.\n*registerServletsV1(ServletConetxHandler context)*:\nHolds a map with the path (as String) and the servlet (as Class). These are then passed to the method \"registerServlets\". In addition we also specify here which version it is. (In this case v1). So you could register different servlets for different API versions.\n\n*registerServlets(ServletConetxHandler context, String version, Map<String, Class<?>> mappings)*:\nHere all servlets of ``mappings`` for the API version ``String: version`` are registered for the ``context``.\nVery simple. And that\'s all we need for the API versioning. From now on our servlet is available at http://localhost:3001/api/v1/service/helloWorld .\n\nIf we want to add another servlet now, we can simply create and pass another one to mappings.put(\"servletName\", Servlet.class) which can be reached e.g. under /health and gives us the health status of the server.\n\n\n## Connect to the Database\nSo far we have a versionable API / MicroService which only gives us \"static\" answers.\nTo process \"real\" data we need a database and have to connect our service to it.\nFor this purpose we first create a MySQL e.g. on remotemysql.com, or locally on our PC.\nSo that our database contains some data you can simply import this sql: \n[Dump20191012.sql](So%20that%20our%20database%20contains%20some%20data%20you%20can%20simply%20import%20this%20sql:%20https://github.com/whit-e/microservice_blueprint_java/blob/master/resources/Dump20191012.sql)\nNow we already have a pool of data.\nOnly the connection to the database is missing.\nFor this I like to create a helper class called ``DBUtils``:\n```java\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.PreparedStatement;\nimport java.sql.ResultSet;\nimport java.sql.SQLException;\nimport java.sql.Statement;\nimport java.sql.Timestamp;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Properties;\n\npublic class DBUtils {\n\n	private Connection connection;\n	private PreparedStatement preparedStatement;\n	\n	public DBUtils() {\n		Properties prop = new Properties();\n		try {\n			// load config.properties for db credentials\n			prop.load(new FileInputStream(\"src/main/resources/config.properties\"));\n			// load jdbc driver and start db connection\n			// mit \"SET GLOBAL time_zone = \"+3:00\" time_zone bei mysql setzen um keine TimeZone errors zu bekommen\n			Class.forName(\"com.mysql.cj.jdbc.Driver\");\n			connection = DriverManager.getConnection(\"jdbc:mysql://\" + prop.getProperty(\"host\", \"localhost\") +  \"/\"+prop.getProperty(\"database\"), prop.getProperty(\"user\"), prop.getProperty(\"password\", \"lol\"));\n			String query = \"INSERT INTO someTable (timestamp, uid, name, text, description) values (?, ?, ?, ?, ?)\";\n			// preparedStatement benefits: faster, no sql injection is possible\n			// must be created once initially with the query and is then only filled with data; this improves performance.\n			preparedStatement = connection.prepareStatement(query);\n		} catch (IOException e) {\n			e.printStackTrace();\n		}\n		catch (SQLException e) {\n			e.printStackTrace();\n		} catch (ClassNotFoundException e) {\n			e.printStackTrace();\n		}\n	}\n	\n	/**\n	 * insert the obj to the database\n	 * \n	 * @param SambleDBObj\n	 * @throws SQLException\n	 */\n	public void insert(SampleDBObj obj) throws SQLException {\n		preparedStatement.setTimestamp(1, new Timestamp(System.currentTimeMillis()));\n		preparedStatement.setString(2, obj.getUid());\n		preparedStatement.setString(3, obj.getName());\n		preparedStatement.setString(4, obj.getText());\n		preparedStatement.setString(5, obj.getDescription());\n		preparedStatement.executeUpdate();\n	}\n	\n	/**\n	 * \n	 * @return List<SampleDBObj> - return all entrys\n	 * @throws SQLException\n	 */\n	public List<SampleDBObj> getAll() throws SQLException {\n		Statement statement = connection.createStatement();\n		ResultSet rs = statement.executeQuery(\"SELECT * FROM someTable\");\n		List <SampleDBObj> objs = new ArrayList<SampleDBObj>();\n		while(rs.next()) {    \n			SampleDBObj obj = new SampleDBObj(rs.getDate(\"timestamp\"), rs.getString(\"uid\"), rs.getString(\"name\"), rs.getString(\"text\"), rs.getString(\"description\"));\n			objs.add(obj);\n		}\n		return objs;\n	}\n\n	/**\n	 * \n	 * @return SampleDBObj - return specific entry by name\n	 * @throws SQLException\n	 */\n	public SampleDBObj getByName(String name) throws SQLException {\n		PreparedStatement statement = connection.prepareStatement(\"SELECT * FROM someTable WHERE name = ?\");\n		statement.setString(1, name);\n		ResultSet rs = statement.executeQuery();\n		if(rs.next()) {\n			return new SampleDBObj(rs.getDate(\"timestamp\"), rs.getString(\"uid\"), rs.getString(\"name\"), rs.getString(\"text\"), rs.getString(\"description\"));\n		}\n		return null;\n	}\n}\n```\nI think what the class is doing should be relatively clear. In the constructor we load the file src/main/resources/config.properties which looks like this: \n```\nhost = localhost\ndatabase = examples\nuser = serviceuser\n```\n\n\nWe connect to our Database and create our preparedStatement for the insert route once.\nThen we have for all our routes (``insert``, ``getByName``, ``getAll``) a corresponding method which e.g. loads the corresponding data via select or insert statement or adds it to the table.\nAs a last step we only have to connect our DBUtils for each route with the SampleServlet so that the corresponding data is loaded from the database and written into the response:\n```java\nimport java.lang.reflect.Field;\nimport java.sql.SQLException;\nimport java.util.Date;\nimport java.util.List;\n\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.QueryParam;\nimport javax.ws.rs.core.MediaType;\nimport javax.ws.rs.core.Response;\n\nimport org.eclipse.jetty.util.StringUtil;\n\nimport com.google.gson.Gson;\n\n\n@Path(\"/\")\npublic class SampleServlet {\n	\n	private DBUtils databaseUils;\n	\n	public SampleServlet() {\n		databaseUils = new DBUtils();\n	}\n\n	// http://localhost:3001/api/v1/service/insert\n	@Path(\"insert\")\n	@GET\n	@Produces(MediaType.APPLICATION_JSON)\n	public Response insert(\n			@QueryParam(\"name\") String name,\n			@QueryParam(\"text\") String text,\n			@QueryParam(\"description\") String description){\n\n		if(StringUtil.isNotBlank(name) && StringUtil.isNotBlank(text) && StringUtil.isNotBlank(description)) {\n			SampleDBObj someObj = new SampleDBObj(name, text, description);\n			try {\n				databaseUils.insert(someObj);\n				return Response.ok(\"Obj successfully created\").build();\n			} catch (SQLException e) {\n				// TODO Auto-generated catch block\n				e.printStackTrace();\n				return Response.status(404, \"An error occurred while calling the service, please try again. \\r\\n\" + e.getMessage()).build();\n			}\n		} else {\n			return Response.status(404, \"Object could not be created, one of the required parameters was empty\").build();\n		}\n	}\n\n	\n	// http://localhost:3001/api/v1/service/getByName\n	@Path(\"getByName\")\n	@GET\n	@Produces(MediaType.APPLICATION_JSON)\n	public String getByName(@QueryParam(\"name\") String name) {\n		if(StringUtil.isNotBlank(name)) {\n		try {\n			return new Gson().toJson(databaseUils.getByName(name));\n		} catch (SQLException e) {\n			e.printStackTrace();\n			return \"An error occurred while calling the service, please try again. \\r\\n\" + e.getMessage();\n		}\n		} else {\n			return new Gson().toJson(\"Object could not be created, one of the required parameters was empty\");\n		}\n	}\n	\n	// http://localhost:3001/api/v1/service/getAll\n	@Path(\"getAll\")\n	@GET\n	@Produces(MediaType.APPLICATION_JSON)\n	public String get() {\n		try {\n			return new Gson().toJson(databaseUils.getAll());\n		} catch (SQLException e) {\n			e.printStackTrace();\n			return \"An error occurred while calling the service, please try again. \\r\\n\" + e.getMessage();\n		}\n	}\n	\n	@Path(\"export\")\n	@GET\n	@Produces(MediaType.APPLICATION_OCTET_STREAM)\n	public Response export() {\n		try {\n			StringBuilder sb = new StringBuilder();\n			for(Field field : SampleDBObj.class.getDeclaredFields()) {\n				sb.append(field.getName()).append(\",\");\n			}\n			// load everything from database\n			List<SampleDBObj> objects = databaseUils.getAll();\n			//header\n			sb.deleteCharAt(sb.length()-1);\n			for (SampleDBObj obj : objects) {\n				sb.append(\"\\r\\n\");\n				sb.append(obj.toCSVLine());\n			}\n			return Response.ok(sb.toString()).header(\"Content-Disposition\", \"attachment; filename=\" + new Date().toString() + \"-export-objs.csv\").build();\n			\n		} catch (SQLException e) {\n			e.printStackTrace();\n			return null;\n		}		\n	}\n}\n```\nInfo: if you get know the Error ``Caused by: com.mysql.cj.exceptions.InvalidConnectionAttributeException: The server time zone value`` pls just run this Query on your Database: ``SET GLOBAL time_zone = \"+3:00\";`` \nWhat is new now is the return type ``Response``. With this we can for example set the Response Code to the \"[HTTP Code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) 200\" - which means the request ran as intended or set any other HTTP Code for our response. \nWith Gson from Google we can convert our objects directly into a JSON without having to worry about parsing some objects.\nAnything else that happens now should be known by now. For some methods we pass our query parameters, check if they are valid, if so the object is created and added to the database or the object with the name XYZ is loaded from the database.\n\nWe\' re done here.\nWe now have a running versioned microservice that queries a database and sends the result as JSON to the client.\n\n### Next Step:   [Dockerizing our Service](https://whit-e.com/articles/dockerizing-our-service)\n\n***Info**/ **best practice**: Why should I always check the database for each request? Just create a **cache**. When the server starts or adds data to the database, it is filled with data. Advantage: significantly less load on the system and your database*\n\n#### Project: [GitHub: microservice_blueprint_java](https://github.com/whit-e/microservice_blueprint_java)','2019-10-15 00:00:00',0,'building-java-rest-api');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Cloud Application #2 Dockerize our Services','Dev, Docker, Container','In the tutorial series \"Cloud Application\" I show you how to build and deploy a MicroService into the Google Cloud. With all buzzwords like \"cloud\", \"api\", \"microservice\", \"docker\", \"ci/cd\" & so on!','## Prerequisites  \n\n- A working MicroService (Tutorial: [Java MicroService]([https://whit-e.com/articles/building-java-rest-api](https://whit-e.com/articles/building-java-rest-api)) or [Node.JS MicroService]([https://whit-e.com/articles/building-nodejs-rest-api](https://whit-e.com/articles/building-nodejs-rest-api)))\n- Docker installed on your server (example on Ubuntu: following Steps 1 and 2 of [How To Install and Use Docker on Ubuntu 18.04](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04))\n- *Optional: a Docker Hub account\n## About the Dockerfile\n### What is the Dockerfile?\nA Dockerfile is a simple text file that describes the structure of our Docker image.\nIt contains multiple different commands, which make sure that the base image is pulled, our microservice gets into the image, ports are monitored and e.g. the application is started.\nAll important commands for this tutorial are described here:\n### Commands:\n#### FROM\nThe most important command in the docker file is FROM. This must be at the top and simply describes which image to use as the basis for our newly created image. This can be basic images like ubuntu or ubuntu:trusty but also images created by users (or ourselves).\n#### RUN\nOne of the most important commands is certainly the ``RUN`` command. This command simply executes the parameter as a shell command, giving us the flexibility to do virtually anything in our virtual machine. The ``RUN sudo apt-get install -y npm`` is translated internally to ``/bin/sh -c sudo apt-get install -y npm``\n\nA second way to execute the RUN command is with the so-called exec syntax. ``RUN [\"executable\", \"param1\", \"param2\"]``\n\nIn many cases our Dockerfiles will be a sequence of many RUN commands. \nInfo: *Each RUN command creates a new layer in the image - consumes memory. That means we should keep the number as low as possible and do several things within one command.*\n\n#### WORKDIR\nEach RUN, CMD, ADD and COPY command starts from the root directory. If we want to change this, the ``WORKDIR`` command works quite simply. It sets the new reference directory for all following commands (or until the next call of WORKDIR).\n``WORKDIR /opt/app``\n\n#### COPY\nCOPY is a command that takes a file or directory from the host system and inserts it into the file system of the image. The syntax is very simple:\n``COPY app /opt``\nFirst comes the directory or file to be copied and second the destination. \n\n#### EXPOSE\nAn important point at Docker is communication inside and outside the containers. To do this, ports must be opened and the ``EXPOSE`` command takes care of that. For example, if we want to run our node service inside a container, we can do this with\n``EXPOSE 3000``\nThis will ensure that port 3000 is monitored.\n\n#### CMD\n``CMD`` provides a container created from our image with a standard command that is executed. This command can of course be overwritten by the container. There can **only** be **one** CMD command per Dockerimage, if several are defined only the last one is valid. From the syntax CMD works exactly like RUN.\n``CMD echo \"visit whit-e.com for more!\"``\n\n## The Dockerfile (Node.js)\nOur new created, empty ``Dockerfile`` is always located in the root directory of the project.\nFor our Node.js application it looks like this:\n```dockerfile\nFROM node:10-alpine\n\n# Create app directory\nWORKDIR /usr/src/app\n\n# Install app dependencies\n# A wildcard is used to ensure both package.json AND package-lock.json are copied\n# where available (npm@5+)\nCOPY package*.json ./\nRUN npm install\n\n# If you are building your code for production\n# RUN npm ci --only=production\n# Bundle app source\nCOPY . .\n\n# Watches for Port 8080\nEXPOSE 8080\n\n# Run the start command, which is defined at our package.json\nCMD [ \"npm\", \"start\" ]\n```\n\nInfo: *If this tutorial is not enough for you and you want a more detailed one for the dockerizing of your Node.js app then you can find one at node too: https://nodejs.org/en/docs/guides/nodejs-docker-webapp/*\n\n## The Dockerfile (Java / Gradle)\nOur new created, empty ``Dockerfile`` is always located in the root directory of the project.\nFor our Java application it looks like this:\n```dockerfile\nFROM gradle:jdk8 as builder\n  \n# Create a new Directory called application\nRUN mkdir application\n\n# Add our gradle files there\nADD build.gradle application\nADD settings.gradle application\n# And add our src directory there\nADD src application/src\n\n# Set the workdir to `application`\nWORKDIR application\n\n# Run gradle for creating a clean jar\nRUN gradle clean build --stacktrace --info\n\n\n# Now we use a clean jre8-slim img only for our application\nFROM frolvlad/alpine-oraclejre8:slim\n# Copy the jar there\nCOPY --from=builder /home/gradle/application/build/libs/Blueprint-latest.jar /app/\n# Set the workdir to /app\nWORKDIR /app\n# Set the entrypoint\nENTRYPOINT [\"java\", \"-jar\", \"Blueprint-latest.jar\"]\n```\nInfo: *If you have another process how to create a jar then you can save yourself the upper process with gradle of course.  Alternatively you can copy your java class files and run the application directly.*\n*In a other tutorial - which will be about CI/CD - we will outsource the building of the jars from the dockerfile to a job in the ci-pipeline.*\n\n## Building your image\n\nGo to the directory/project that has your  `Dockerfile`  and run the following command to build the Docker image. The  `-t`  flag lets you tag your image so it\'s easier to find later using the  `docker images`  command:\n\n```bash\ndocker build -t <your username>/node-api .\n# or for the java file\ndocker build -t <your username>/java-api .\n```\n\nYour image will now be listed by Docker:\n\n```bash\n$ docker images\n\n# Example\nREPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE\n<your username>/java-api     latest              f1696fc64aa8        3 minutes ago       136MB\n<your username>/node-api     latest              7e5548e3f5de        33 minutes ago      88.9MB\ngradle                       jdk8                951aca94658f        36 hours ago        553MB\nnode                         10-alpine           b95baba1cfdb        2 months ago        76.4MB\nfrolvlad/alpine-oraclejre8   slim                732243639a95        9 months ago        123MB\n```\n## Run your Image\n\nRunning your image with  `-d`  runs the container in detached mode, leaving the container running in the background. The  `-p`  flag redirects a public port to a private port inside the container. So if our application normally runs on port 3000 or 3001, we can redirects them to 8080.  Inside the container the application will run on the defined app port (e.g. 3000) but outside the container we can access port 8080 via the host. Docker handles the port mapping for us hereRun the image you previously built:\n\n```bash\n\n#docker run -p publicPort:innerPortOfApplication -d <your username>/node-api\ndocker run -p 8080:3000 -d <your username>/node-api\ndocker run -p 8081:3001 -d <your username>/java-api\n```\n\nPrint the output of your app:\n\n```bash\n# Get container ID\n$ docker ps\n\n# Print app output\n$ docker logs <container id>\n\n# Example\nRunning on http://localhost:8080 # -> our nodejs api\nRunning on http://localhost:8081 # -> our java api\n```\n\nIf you need to go inside the container you can use the  `exec`  command:\n```bash\n# Enter the container\n$ docker exec -it <container id> /bin/bash\n```\n\n## Access to our local MySQL\nOur docker containers are now running smoothly and we can also call our services under the appropriate paths.\nHowever, we use a local MySQL in which the corresponding data is stored. To access this data we have to change the host of the database from: ``localhost`` to ``host.docker.internal``\nInfo: *If this doesn\'t work for you, you have to find out the local ip of your host with ``ifconfig`` and use it accordingly (which would also be my recommendation, because then your service will continue to run locally without any different settings)*','2019-10-20 00:00:00',0,'dockerizing-our-service');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Cloud Application #3 Infrastructre as Code - Create K8 Cluster on GCloud with Terraform & Deploy Container','Operations, Terraform, Docker','In the tutorial series \"Cloud Application\" I show you how to build and deploy a MicroService into the Google Cloud. With all buzzwords like \"cloud\", \"api\", \"microservice\", \"docker\", \"ci/cd\" & so on!','You\'re thinking about your last game of Buzzword Bingo when you read the headline? I trust you immediately!\nBut here I explain to you - what exactly is \"Infrastructure as Code\" using the examples of Terraform, Kubernetes and the Gcloud.\n\n## WHAT is ``infrastructure as code``\n\nFirst of all, it\'s more than just a buzzword that sounds fancy on presentations!\nInfrastructure as Code means that a infrastructe is the result of executable code. This code can then be executed, duplicated, automatically deployed or deleted in the cloud. This approach uses the potential of the cloud very intensively. Because of that, systems could be created and scaled in the shortest possible time.  Infrastructure as Code makes the following points possible: \n- Centralized management of source code with versioning.\n- High transparency\n- Automated testing of server and virtual machine configuration\n- Automated testing of deployments\n\nIt also has the advantage that anyone can simply delete and create a cluster that is always in the defined state - not like dedicated hardware!\nWhat previously was a manual process is now automated and more efficient than ever. With the \"Infrastructure as Code\" concept, a company can quickly set up new servers, automate development cycles and respond quickly to new business requirements. In my opinion, Infrastructure as Code is part of every good cloud solution. \n\n## Terraform Kubernetes Setup:\n### Requirements:\n- Google Account\n	- *if you don\'t have a cloud account yet: a credit card (to get free access to the Google Cloud)\n- Terraform ([How to install terraform](https://learn.hashicorp.com/terraform/getting-started/install.html))\n\n### Create a new GCloud Project\nBecause I expect that you haven\'t created a project yet, or your existing one is already configured somehow, we simply create a new project.\nTo do this, simply click on this link: [https://console.cloud.google.com/projectcreate](https://console.cloud.google.com/projectcreate)\nAs the project name we take something suitable, just like the project ID. In our case, because it is an example tutorial, the project is called: \"cloud-application-tutorial\".\nAfter we have created the project, we start with configuring the cloud provider.\n\n### Configure the Cloud Provider\nSo that we can authenticate ourselves against the Google Cloud, we have to generate a JSON with the corresponding access data once. This works as follows:\n1.  Log in to the  [Google Developers Console](https://console.developers.google.com/)  and select the project you would like to obtain the credentials file for\n2.  Go to the “API Manager” through the hamburger menu on the left-side\n3.  Now click on “Credentials” on the left-side\n4.  Click the “Create credentials” button and select “Service account key”  and choose “JSON” as the format. (If you have to create a new \"Service Account\" - sei dir sicher das er ausreichende rechte hat. Ich habe als Beispiel jetzt gesagt der Service Account ist Bearbeiter des gesamten Projektes)\n5.  Click on “Create” to generate and download the key (save it to a new Directory called \"terraform-cloud-application\").\n\nSo that we can authenticate our system against the Google Cloud, we have to generate a JSON with the required credentials once. The next step is to configure our cloud provider for Terraform.\n*important here is that our ServiceAccount.Json is located in the same folder as our terraform.file, or you store the credentials as an environment variable. \nBut please **NEVER PUSH CREDENTIALS TO A PUBLIC REPO!!!**\nNow let\'s create our cloud-provider file, called``providers.tf``:\n```\nprovider \"google\" {  \n	credentials = \"${file(\"account.json\")}\"  \n	project = \"${var.gcloud-project}\"  \n	region = \"${var.gcloud-region}\"  \n}\n``` \n\n### Create some Default-Variables\nIf you are wondering about where we got the \"var.gcloud-project\"-param at our providers.tf file... the answer is: currently we don\'t know it yet!\nBecause of that we have to create a file called ``variables.tf`` which contains them:\n```\nvariable \"gcloud-region\" { default = \"europe-west1\" }  \nvariable \"gcloud-zone\" { default = \"europe-west1-b\" }  \nvariable \"gcloud-project\" { default = \"cloud-application-tutorial\" }  \nvariable \"platform-name\" { default = \"cloud-application-platform\" }\n```\n**Info**: *If you create the files with VS code, you will be shown how often your variables are used. In this way you can sort out unnecessary variables later.*\n\n### Network Basic Structure\nNext we create a global network for our platform. *In this network, or rather its firewall, we determine which IP\'s have access to our application and which ports are open for these IP\'s.*\nFor our global network we create the file ``global.tf`` and define the network there as follows:\n```\nresource \"google_compute_network\" \"platform\" {  \n	name = \"${var.platform-name}\"  \n}\n```\nAs already said we need a firewall to restrict the traffic to http (port 80), https (port 443) and two custom ports (8080, 8081). The following is applied to our network above and only allows the protocol \"tcp\" with the defined ports. Through the ``source_ranges = [\"0.0.0.0/0\"] `` we allow all external traffic. For the firewall we add the following to the ``global.tf``:\n```\nresource \"google_compute_firewall\" \"ssh\" {  \n	name = \"${var.platform-name}-ssh\"  \n	network = \"${google_compute_network.platform.name}\"  \n	allow {  \n		protocol = \"tcp\"  \n		ports = [\"80\", \"443\", \"8080\", \"8081\"]  \n	}  \n	source_ranges = [\"0.0.0.0/0\"]  \n}\n```\n**Info**: *To access our network more easily, it is a good idea to register a domain name. For this we add the following:*\n```\nresource \"google_dns_managed_zone\" \"sample-platform\" {  \n	name = \"endless-beer\"  \n	dns_name = \"endless.beer.\"  \n	description = \"endless.beer DNS zone\"  \n}\n```\n*In the above example we’re setting up a DNS zone for the domain name “endless.beer” (yeah it\'s one of my websites :P). Of course, this will only work if you actually set the domain’s nameservers to the nameservers Google provides after you create the DNS zone by applying the Terraform code.*\n\n### Create our K8-Cluster\nWe could now create several clusters for our network above. Depending on the company structure, it might be a good idea to have a development, test and production cluster. Of course this can be done individually. If you maybe work with canary releases, a production cluster is quite enough. Each cluster is defined in its own terraform file. That could be ``develop.tf``, ``qa.tf``, ``staging.tf``, ``production.tf`` or something like that.\nI would like to show you how to create a cluster, in our case the \"production\" cluster. \nFirst we create the sub-network for the cluster. All configurations are stored in a file called: ``prod.tf``.  \n```\nresource \"google_compute_subnetwork\" \"prod\" {  \n	name = \"prod-${var.platform-name}-${var.gcloud-region}\"  \n	ip_cidr_range = \"10.1.2.0/24\"  \n	network = \"${google_compute_network.platform.self_link}\"  \n	region = \"${var.gcloud-region}\"  \n}\n```\nEverything we define here for our subnet is: \"the name\", \"the IP range\", \"the global network\" (in which the subnet is located), \"the region\". \nIt is important here that each network has its own IP range. Overlaps can cause problems when creating your subnetwork. (``terraform plan`` then thinks \"the file looks correct\" and ``terraform apply`` creates everything up to the collision of the networks and then aborts... which means that you have to manually delete everything, at the worst case). \nOur next ip_cidr_range would be e.g. 10.1.3.0/24 because 10.1.2.0/24 contains the IPs from 10.1.2.0 to 10.1.2.255.\nSo after we have created our subnet, we create the k8 cluster.\nHere we define things like the initial Node Count (1), the [Node-Machine-Type] (https://cloud.google.com/compute/docs/machine-types) and our network settings (global network, subnet, and zone):\n```\nresource \"google_container_cluster\" \"prod\" {  \n	name = \"prod\"  \n	network = \"${google_compute_network.platform.name}\"  \n	subnetwork = \"${google_compute_subnetwork.prod.name}\"  \n	zone = \"${var.gcloud-zone}\"  \n  \n	initial_node_count = 1  \n  \n	node_config {  \n		machine_type = \"n1-standard-1\"  \n	}  \n}\n```\n\nWe\'ve finished everything up to here! Next:\n### Apply Terraform Code\nIf you are starting terraform for the first time, please run \"terraform init\". This will initialize all required providers (e.g. gcloud) once.\nBefore we apply our infrastructure / code into the cloud, we execute ``terraform plan``. ([Terraform Doc](https://www.terraform.io/docs/commands/plan.html) ).\n\nWhen running the _plan_ command, Terraform will perform a refresh and determine which actions are necessary to achieve the desired state as specified in our code. \n\nBut now run it! \n```bash\nterraform plan -out prod.plan\n```\nWe save the plan by specifying the _-out_ parameter which ensures that when we run _apply_ only the actions in this plan are executed.\nBefore we apply our infrastructure: *if you dont configure your gcloud yet, you have to enable the Compute Engine API: https://console.developers.google.com/apis/library/compute.googleapis.com?project=yourProject and the Kubernetes Engine API: https://console.developers.google.com/apis/library/container.googleapis.com?project=yourProject*\nLet’s now run ``apply`` to actually create our infrastructure.\n\n```\nterraform apply\n```\n**Info**: *The apply and build of our new infrastructure can take a few minutes*\n\nThe sources for our Terraform example are available here:\n[https://github.com/whit-e/terraform-cloud-application](https://github.com/whit-e/terraform-cloud-application)\n\n## Connect to the Cluster\n### Install required Tools:\n>1.  [Install the Google Cloud SDK](https://cloud.google.com/sdk/docs/quickstarts), which includes the  `gcloud`  command-line tool.\n>2.  Using the  `gcloud`  command line tool, install the  [Kubernetes](https://kubernetes.io/)  command-line tool.  `kubectl`  is used to communicate with Kubernetes, which is the cluster orchestration system of GKE clusters:   \n>    gcloud components install kubectl\n>3.  Install  [Docker Community Edition (CE)](https://docs.docker.com/engine/installation/)  on your workstation. You will use this to build a container image for the application.\n>4.  Install the  [Git source control](https://git-scm.com/downloads)  tool to fetch the sample application from GitHub.\n\nQuote from: [https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app](https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app)\n\n### Build & Push Docker Images to the Cloud\n#### build\nNext we take one of the docker images we created here ( [https://whit-e.com/articles/dockerizing-our-service](https://whit-e.com/articles/dockerizing-our-service) ) and push it into the Google Cloud.\n\n**Attention**: *We have to build our container as follows (because it will be placed in the Google Registry and there are some conventions here)*:\n```\nexport PROJECT_ID=yourProject\ndocker build -t gcr.io/${PROJECT_ID}/node-api:v1 .\n```\nSince we have already worked with the containers in the previous [tutorial](https://whit-e.com/articles/dockerizing-our-service), we go directly to the next step:\n#### push\nFrom Google - https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app#step_2_upload_the_container_image \n> You need to upload the container image to a registry so that GKE can download and run it.\n> First, configure Docker command-line tool to authenticate to  [Container Registry](https://cloud.google.com/container-registry)  (you need to run this only once):\n> ``gcloud auth configure-docker``\n>You can now use the Docker command-line tool to upload the image to your Container Registry:\n> ``docker push gcr.io/${PROJECT_ID}/node-api:v1``\n\nIf everything works, we could see our image here: https://console.cloud.google.com/gcr/images\n### Create a Container Deploymen\nWe can now deploy our container very easily via shell command or we can apply a deployment.yaml file. \n**Info**: *You must have [kubectl installed](https://kubernetes.io/de/docs/tasks/tools/install-kubectl/) and you must be in the corresponding Google Cluster*.\n####  via Shell Command\nHere you must first specify \"What we create\", \"the name\" and the \"image of our container\". \n```shell\nkubectl create deployment node-api --image=gcr.io/${PROJECT_ID}/node-api:v1 \n```\n#### via deploy.yaml\n```yaml\napiVersion: extensions/v1beta1  \nkind: Deployment  \nmetadata:  \n	name: node-api   \nspec:  \n	replicas: 1\ntemplate:  \n	metadata:  \n	labels:  \n	app: node-api  \nspec:  \n	containers:  \n		- name: node-api  \n		image: eu.gcr.io/yourApplication/node-api:v1\nimagePullPolicy: Always  \nresources:  \n	limits:  \n		cpu: 20m  \n		memory: 128Mi  \n	requests:  \n		cpu: 10m  \n		memory: 64Mi  \n```\nNow you could apply the deploy.yaml with the following command: \n``kubectl apply -f deploy.yaml``\n\n#### Port-mapping\nSo that we can access to our application (from outside) we need a simple port mapping:\n```\nkubectl expose deployment node-api --type=LoadBalancer --port 8080 --target-port 3000\n```\n--port : is the port which is opened to the external traffic.\n--target-port : is the port which is used by our application (defined at bin/www at our node.js app).\n\n\nThat\'s it! Google itself also has a VERY good guide on how to deploy a container (if mine isn\'t enough for you):  [https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app](https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app)\n\n\nWe have now successfully created a cluster with Terraform and then pushed a container into the Google Registry and deployed it manually.\n\n__________\n*The sources for our Terraform example are available here:*\n[https://github.com/whit-e/terraform-cloud-application](https://github.com/whit-e/terraform-cloud-application)\n\nIf you have any questions, please feel free to write a comment. Even if something doesn\'t work anymore (software becomes more up-to-date and some methods outdated) just write!\n\nIn the next step we look at: \"Deploy it Continuously to the Cloud\"!\n','2019-10-21 00:00:00',0,'create-kubernetes-with-terraform');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Create a Custom *.onion URL','Operations','You have your own Tor Hidden Service running but you are still missing the right URL? Here I explain how to generate a *. onion domain with the help of Shallot. I also offer a small script to speed up the generation process.','It is very simple to create your own domain for the Tor network. -> If you don\'t know how to install the Tor service and host a site there, read the following article:  [tor-hidden-service-nginx](https://whit-e.com/tor-hidden-service-nginx)\n\n### Step 1 - Download Shallot\n\nChange to your opt folder`cd /opt`Now there are two ways to get Shallot.\n\n#### Via Git\n\n`git clone https://github.com/katmagic/Shallot.git`\n\n#### Via Browser\n\n`curl https://github.com/katmagic/Shallot/archive/master.zip`\n\n### Step 2 - Install Shallot\n\nInstalling Shallot is also very easy, just type the following commands in your terminal:\n\n```\ncd /opt/Shallot/\n./configure && make\n./shallot \n\n```\n\n### Step 3 - Generate your *.onion URL\n\nTo generate a *. onion url now, we only have to execute shallot and give our desired url as parameter. You should always keep in mind the times for the url generation, these are linked on the  [github](https://github.com/katmagic/Shallot)  page of the project.\n\n```\n$  ./shallot ^white\n------------------------------------------------------------------\nFound matching domain after 44181553 tries: whitejqzenjmyya7.onion\n------------------------------------------------------------------\n-----BEGIN RSA PRIVATE KEY-----\nMIICXgIBAAKBgQDO7hI7U6ZuH/4Has0TzMAoH+n36TvHyjdSLKrzU90StlrkoQFf\nw6ZZHXFYW5R7M9J43M+L03bJ8mOlUUTby4NqkJ39hKYuY/fRKFmNTEzbCiR3Hq/P\nHjfXvd9CeZQFA8JfILpWfQNTuZcMMDGS0a/leuknEab0EzDq6lEBsHXPWwIEAWJg\niwKBgBxsOPzixRFXXZxl1nEbQHVPQCOh/bBNeUiY78TkHHbFDJatww2/Bu88JYEb\nhAfUV61xKNcPu0CS/Lu0JSxBo60f9Q7tUv10q3bl9cz/X/3Beb5fW9sCOFVEnCH8\nZo66AZOX219YsrCV0MU+82KGH8ckOvIhxPNMgyIUB9DzBJATAkEA9ODPaAsUMQcu\ndliegijag9q7tQCAuvSsjm/TTsEwCdsuGMIxrfL0FP9w23PbJI6i1JNVRxHZ9BU9\nwr39haPE9QJBANhUCeWN8C608S6OmwSRwLrrNZxo75Fwy4Dw8dyfA7Uy9lA9Fyj8\nmgyQ5cHCyZ0OpGii57odb7s/i1H4hbcFEQ8CQQCaeV7uQs76EWtm62e8or9Hb9qW\nr7Hu3rIpt9+jljIlupZ3SYOSaxVy8kQgP52S+dQj9QLn/H9yc/QbGAu+SdrXAkBA\n93cCmDbkHeeoA2u1MPjDC9BHunqVGi4U0NIApwrQEa0xL0RIsqBk4I5JHwyTEOE/\ntFKxsc37ty5/E/klFaWNAkEA7sg3tJz5qJ9fN/JXneSPqokqiIkBxNcHzmFccPAY\n8H9f55cHwlfJzR1zS7K0dGpOX4J43sjKqViUVSMHwS4QKA==\n-----END RSA PRIVATE KEY-----\n\n\n```\n\nAfter a matching url has been found, we only have to replace the existing private_key in our Tor directory (/var/lib/tor/example_page) with the new one. And finally we restart Tor and can now reach our site via the new domain.\n\n### Optional\n\nBecause the generation of larger addresses takes more time. I have written a small shell script which allows us to save all generated urls. In other words, if I want a URL that starts with \"whitecom\", I can give one ^white as a parameter - so all created urls start with white. And as soon as I have one that starts with \"whitecom\" I stop creating urls. This gives me the possibility to generate several suitable domains and to choose the best one at the end, because all private_keys are stored in the /tmp folder.\n\nHere a short example:\n\n```\n./addressgenerator.sh ^whi white\n---------------------------------------------------------------\nFound matching domain after 65819 tries: whitedofszgmqnoq.onion\n---------------------------------------------------------------\n-----BEGIN RSA PRIVATE KEY-----\nMIICXAIBAAKBgQDEl0aii2Uh2p1S+4LymgvTSE2FxgyBASu7ke3WydBR6PEXwfNh\n/9GMxpOms9TeuHRneHPntie0oBd01fh6Lfk/pSs+BViiqQqfkvIgK6cRYl1nSDaO\nhOzKKwL4/HEFCKD5sav89oGrkZSryV94ipkY7IkdIwwP5a4FugAX0G1VTwIDAwI1\nAoGAAXb0LSOzXXylIBxvfsfFBVeJO6f9O/OmU0nWJA6lPzsoCrHobonLnQZqxokj\nFgYkTJ2M0xjowrXqYsR8DwKoUJTFMCpKqt8rideB/vcbn1HtfqxV6f6CaHd0V8RQ\ngtmZOIm3u7Xzw7uGcbhk681L/I/8m4kZvT4ywZHtqS7LJB0CQQDjiZoX3dMJEIgk\nsAAhCTJYVWTNap2Z7UX2/Nfea6W0ylJj3NvLepoo9pKJoVhYXlpYSY0F5Ln6LYZG\noDFInUNPAkEA3S6tIgyaMGro8hI8cLIZhbWMv3E3rX4sCWLTzbebWRUUlOKJWyYN\n0zvU6CIhiS30XOZRGmZTyI3KCZW8klJOAQJAFehTbkfRt+nDq6znpMZity1nF4io\niQ1i6wEBj9vKNkmjoofpXGAehtoEkjJ+b28MQdyQ0gnTRV1i8K1i3n6VjQJBAL7m\nvXSvspX2ZvC00IgJLzJciAuAst7g1l5B0XXgQfLyHVyuepTVjNgwpnnzZkONMxAh\n2QgV5FBet6oiZWrGwh0CQFvPgz6zyR0QFtftvtSwemgDlK7qk+v1nGclZNf6fgJk\nI1hC7rXLmkDesIsIdqPZ5NQV8/HfpEIXWS1AXCrR4B4=\n-----END RSA PRIVATE KEY-----\n\n```\n\nI have now generated about 5-10 domains which all started with whi. Domains with the beginning of \"whit3\" and so on could have been there. When the domain finally started with \"white\" I stopped generating new ones and displayed the appropriate Private_Key. As a result, I was able to notice a significant reduction in the time taken to generate new domains.\n\nMy small script for address generation can be found  [here](https://github.com/whit-e/OnionAddressGenerator). It must be in the same folder as Shallot and you should create a tmp folder there.','2017-12-26 00:00:00',0,'custom-onion-url');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('ElasticQueryTester (Website - non responsive)','Dev, HTML','To test Elastic Querys fast and easy and also against different systems I built the ElasticQueryTester. On the example page this goes against my crawled Wiki article and shows elements of it.','wiki-crawler-example','2018-05-10 00:00:00',0,'wiki-crawler-example');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('HOW TO: Fedora looks & feels like Mac OS X Mojave','Linux, Fedora','You like the design of OS X, like functionalities like gesture control but don\'t want to buy a laptop with a broken keyboard for 2000€? How about Fedora on your current laptop + a little \"Gnome Magic\"?','\n## Introduction\nIn this tutorial I will show you how to get some features and the layout of OS X on your Fedora.\n\n### For the Hater who will say \"You want it to look like a Mac, buy one\".\nI know there are some who will say \"if you want it to look like os x, buy a MacBook\",  some of my work colleagues reacted in the same way... But, I have 2 MacBooks (1 MacBook Pro mid 2011, 1 MacBook Pro mid 2018). The 2018 is from work and just bad. With the keyboard, the keys regularly lock up and the speakers randomly start noisy. And I don\'t buy it for 3000€ electronic trash!\nI\'m completely satisfied with my Mid2011 MacBook, but it\'s no longer updateable, so I bought a Lenovo for programming on the couch. But for the development Windows is out of question -> that´s why i started using Fedora. And here I would not like to do without some features I am used to as a long time Apple user therefore this tutorial!\n\n## Prerequisites\n- Fedora 29 or 30\n- Gnome 3\n- Small Linux KnowHow\n\n## Design before Functionality\nFollowing Apple\'s motto - design before functionality (otherwise the new MacBooks wouldn\'t be so buggy) we start with the look, the appearance.\nIn order to be able to make different settings on ``Gnome`` we first have to install ``Gnome-Tweaks``\n```shell\nsudo dnf install gnome-tweaks\n```\nAfter we install tweaks, pls restart your System. Or just log out and in.\nNext, we need to download our desired OS X themes. Here you can find the icon pack \"[XONE_REBORN](https://www.gnome-look.org/p/1218021/)\" the theme \"[Mojave-dark](https://www.gnome-look.org/p/1275087/)\" and the shell theme \"[mcOS11-Shell.zip](https://www.gnome-look.org/p/1220826/)\". \nAfter downloading the files, we extract the content and move it to /usr/share/themes or icons:\n```shell\nsudo tar -xvf Mojave-dark.tar.xz --directory /usr/share/themes/\nsudo tar -xvf XONE_REBORN.tar.xz --directory /usr/share/icons/\nsudo unzip mcOS11-Shell.zip && mv mcOS11-Shell /usr/share/themes/\n```\nNext we download the Apple Typical font \"[roboto](https://www.fontsquirrel.com/fonts/roboto)\", unzip the content and install the fonts (just double click on the .ttf file):\n```shell\nsudo unzip roboto.zip\n```\nNow we have all the files, themes, icons, fonts and so on we need for a nice OS X UI under Gnome3 & Fedora. Next we have to configure Gnome. For this we start Gnome tweaks.\nIn the next screenshots you can see which things have to be configured:\n1. ``Extensions`` > ``User Themes`` > ``enable`` -  for your Shell Theme![](https://whit-e.com/images/screenshots_osx_fedora/extensions_user_themes.jpg)\n	2. For Appearance we have to make changes to ``Applications``, ``Icons`` and ``Shell``:![](https://whit-e.com/images/screenshots_osx_fedora/appearance_settings.jpg)\n	3. For the correct font we have to set ``Interface Text`` & ``Legacy Window Titles`` to ``Roboto Bold`` and ``Document Text`` to ``Roboto Regular``![](https://whit-e.com/images/screenshots_osx_fedora/fonts.jpg)\n	4. As last step we only have the Window Titlebars. Here we activate the titlebar buttons \"Maximize\" and \"Minimize\" and set the Placement to \"left\". ![](https://whit-e.com/images/screenshots_osx_fedora/window_titlebars.jpg)\n\nNow your Fedora should looks like a Mac OS X! \n## Let´s add the ``functionality``\nHere is a short overview of what we will add:\n- [The dock](https://extensions.gnome.org/extension/307/dash-to-dock/)\n- [Gesture control](https://extensions.gnome.org/extension/1253/extended-gestures/) (change the workspace with 3 fingers, for example) \n- [Windows in the full screen has no titlebar](https://extensions.gnome.org/extension/1267/no-title-bar/) (doesn\'t work with all applications) \n- [Windows in Maximize gets its own workspace](https://extensions.gnome.org/extension/1181/maximize-to-workspace/).\nInfo: *All these functionalities are installed directly via Gnome Extensions. If you already have a Chrome on your Fedora, I recommend to use this chrome extension [Gnome Shell-Integration](https://chrome.google.com/webstore/detail/gnome-shell-integration/gphhapmejobijbbhgpjhcjognlahblep) to avoid the manual installation and it works much better with this extension, because it analyzes your current Gnome & Fedora version. After the installation of your extensions, i would recommend to restart your operating system.*\n ### Manual Extension Installation\n #### Extended Gestures\n ```shell\n git clone https://github.com/mpiannucci/gnome-shell-extended-gestures\nsudo cp -r gnome-shell-extended-gestures/extendedgestures@mpiannucci.github.com /usr/share/gnome-shell/extensions\n ```\n#### Maximize to Workspace\n```shell\ngit clone https://github.com/rliang/gnome-shell-extension-maximize-to-workspace ~/.local/share/gnome-shell/extensions/maximize-to-workspace@rliang.github.com\ngnome-shell-extension-tool -e maximize-to-workspace@rliang.github.com\n```\n#### No Title Bar\n```shell\nsudo dnf install xorg-x11-utils\ngit clone https://github.com/franglais125/no-title-bar.git\ncd no-title-bar/\nmake install\n```\n#### Dash to Dock\n```shell\ngit clone https://github.com/micheleg/dash-to-dock.git\ncd dash-to-dock/\nmake install\n```\nNow you can configure your gestures and other extensions in tweaks. \nI hope you enjoy your Fedora with the OS X look.\n\nPS. which currently still annoys me, maximize only creates a new workspace but doesn\'t open the application in fullscreen (f11 shortcut).\nIf necessary, I will soon write my own Gnome extension for this. If you are interested in it, please write it in the comments and check my [Github](https://github.com/whit-e) account regularly, maybe it is already there when you read that tutorial ;) ','2019-10-18 00:00:00',0,'fedora-looks-like-mac-os-x-mojave');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Host a Tor Hidden Service (Nginx)','Operations, Nginx','You don\'t want your site to be visible everywhere? And you don\'t want to use the upload filter... Here I show you how to host a website in the Tor network!','### Prerequisites\n\nWhether you want to create a new website or host an existing one on the Tor network, you will need the following packages in both cases:\n\n**Tor**:\n\n`apt-get install tor -y`\n\n**Nginx**:\n\n`apt-get install nginx -y`\n\n### Step 1 - Create Website\n\n_If you already have an existing page you can skip this step._\n\nFirst of all, we need a simple website. For this we create a directory for the page in /var/www.\n\n`mkdir -p /var/www/example_page`\n\nIn this folder we now create our page which is supposed to be accessible via the Tor network.\n\n`nano /var/www/example_page/index.html`\n\nOur HTML file now gets a little content so that we can see if we are really on our server.\n\n```html\n    <!DOCTYPE html>  \n       <html>  \n           <body>\n               <h1>Congratulations!</h1>\n                <p>You have successfully set up your Tor Hidden Service!</p>\n            </body>\n        </html> \n\n```\n\nVery good! Step 1 is now done, next we will configure our Nginx service!\n\n### Step 2 - Setup Nginx\n\nFirst we need a vhost file for our website.\n\n`nano /etc/nginx/sites-enabled/example_page.vhost`\n\nWe now fill up the file with this configuration:\n\n```\nserver {  \n    // localhost and your port\n    listen   127.0.0.1:80; \n    // root directory of your website\n    root   /var/www/example_page;\n    // index.html of your website\n    index  index.html;\n}\n\n```\n\n_**Attention**! If you use the parameter \"server_name\" in your nginx configuration, you also have to add your *. onion url here (how to get it you can find out in step 3)_\n\nThat is the basic vhost File. Now we just have to validate our vhost-File and restart Nginx.\n\n`nginx -t``service nginx reload`\n\n**Optional**:  _If you are using for example a Node. js server, you only have to change the port from 80 to your server\'s port and it will work in the same way._\n\n_You also can say`deny all`and just`allow 127.0.0.1`then your page is just on the Tor Network available._\n\n### Step 3 - Configure the Tor Hidden Service\n\nOpen the tor configuration file:\n\n`nano /etc/tor/torrc`\n\nand create there your Hidden Service:\n\n```\nHiddenServiceDir /var/lib/tor/example_page/  \nHiddenServicePort 80\n\n```\n\nnow just restart Tor\n\n`service tor restart`\n\nand your site is available on the Onion Network.\n\n_To find out the current URL of your page change to the Tor folder of your page_\n\n_`cd /var/lib/tor/example_page/`_\n\n_and enter the following command here:_\n\n_`cat hostname`_\n\n----------\n\nHave fun with your site! I hope the article could help you.','2017-12-22 00:00:00',0,'tor-hidden-service-nginx');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Host your own VCS Server','Operations','Are you working on a top secret project where you want to have full control over the sources, but still need a versioning system so that your files are stored in at least more then one locations (local and server)?\nI\'ll explain how you can quickly and easily set up your own VersionControlSystem.','### Why should I host my own Version Control System Server?\n\nIn times of cloud storage, Github, Bitbucket & Co. nobody will worry about hosting their sources. It goes very quickly, you create a project, commits your sources and there\'s a Private_Key in Github\'s public repo. Unfortunately, very few people know how to delete their history, so this key will be there forever - you think nobody is that stupid? Take a look at  [https://github.com/search? l=JavaScript&q=secretkey%3D&type=Code&utf8=%E2%9C%93](https://github.com/search?%20l=JavaScript&q=secretkey=&type=Code&utf8=%E2%9C%93)\n\nNow you can see how carelessly data is handled today.\n\nImagine you have a project, an idea that is considering making money because it\'s so unique, so why put this project in the hands of a stranger hoster?\n\nThat\'s why I\'ll show you two options how to host your code on your own server.\n\n### Option A - Gitea\n\nFirst - why is Gitea Option A? Now Gitea is a Fork of Gogs, which is much more active than the original project. One of the reasons for this is that Gogs was originally developed by only one person and  [Gitea is a community project.](https://blog.gitea.io/2016/12/welcome-to-gitea/)  It is one of the lightest version control systems and would even run on a Raspberry Pi.\n\nIf you are interested in testing it you have the possibility to do so here:  [https://try.gitea.io/](https://try.gitea.io/)  - I have created an account there which is called \"test.whit-e\" and has as password \"tester\". You are welcome to use it, but don\'t change the password for fairness, please.\n\n#### Installation\n\nInstalling Gitea is very easy. Assuming that you have already installed git, this is not the case you can do this as follows:`sudo apt-get install git -y`Next we change to the folder where Gitea should be installed, I created the folder  **gitea**  in /**opt**.\n\n```\nmkdir /opt/gitea\ncd /opt/gitea \n\n```\n\nThere we download the  [latest version of Gitea](https://dl.gitea.io/gitea/)  with wget and give it the necessary permissions:\n\n```\nwget -O gitea https://dl.gitea.io/gitea/1.3.2/gitea-1.3.2-linux-amd64\nchmod +x gitea\n\n```\n\nNext we can start Gitea and continue the installation via the browser. There you have to enter the permissions, MySQL data, folders etc. that apply to you.`./gitea web`\n\n**Attention**: By default, Gitea runs on port 3000 when you start it and get the following error message:`[..... io/gitea/cmd/web. go: 156 runWeb ()] [E] Failed to start server: listen tcp 0.0.0.0:3000: bind: address already in use`\n\nyou can easily change the default port. Just edit the file in  _**/opt/gitea/custom/conf/app.ini**_  and add the following lines to it:\n\n```\n[server]\nHTTP_PORT        = 5000\n\n```\n\nIf the app. ini file does not exist yet, start Gitea and it will be generated automatically.\n\n### Option B - Gogs\n\nThere are several ways to install gogs. The installation is basically the same as with Gitea. The best thing to do is to visit the site and choose the most suitable one ;)  [https://gogs.io/docs/installation](https://gogs.io/docs/installation)\n\n-   [install from binary](https://gogs.io/docs/installation/install_from_binary)\n-   [install from sources](https://gogs.io/docs/installation/install_from_sources)\n-   [install from packages](https://gogs.io/docs/installation/install_from_packages)\n\n### Option C - Gitlab\n\n#### Systemrequirements\n\nThe published  [GitLab hardware requirements](http://docs.gitlab.com/ee/install/requirements.html#hardware-requirements)  recommend using a server with:\n\n-   2 cores\n-   4GB of RAM\n\n#### Dependencies:\n\nIn the beginning we have to install some dependencies to get Gitlab up and running. These can easily be pulled and installed from the Ubuntu standard package repository:\n\n```\nsudo apt-get update\nsudo apt-get install ca-certificates curl openssh-server postfix\n\n```\n\n#### Installation\n\nAfter installing the dependencies, we can now take care of installing Gitlab.\n\nTo do this we have to download the  [installation script](https://packages.gitlab.com/gitlab/gitlab-ce/install)  first. It is recommended to switch to the /tmp folder first.\n\n```\ncd /tmp\ncurl -LO https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh\n\n```\n\nNext, we\'ll run it:`sudo bash /tmp/script.deb.sh`\n\nBut what does this script actually do? It makes sure that we can use Gitlab just like other packages via \"_**apt**_\" - our standard package management - can install. And this is what we\'re going to do now:`sudo apt-get install gitlab-ce`After the Gitlab is installed, it would theoretically be available. But I recommend to take a look at the \"/etc/gitlab/gitlab/gitlab. rb\" and to register your domain with external_url:`sudo nano /etc/gitlab/gitlab.rb`\n\nSave and close the file. Then run the command to reconfigure Gitlab:`sudo gitlab-ctl reconfigure`\n\n#### Login\n\nNow our Gitlab is available. We can log in with root and the corresponding password! It is useful to get an overview of the available functions. These are for example: changing your user name, assigning a new password, creating groups, etc. That\'s the best way to learn!\n\n----------\n\n## Conclusion\n\nI have already had good experiences with all 3 systems, but I am currently using Gitea. Personally, it\'s just what I like best. As already mentioned, it is written in Go and incredibly lightweight. Even on a raspberry pi, it will run smoothly.\n\nThere are certainly some other good self hosted VCS, but I don\'t have any experience with them yet, so I don\'t want to go into them in more detail. But you can find a big listing here:  [https://alternativeto.net/software/gitlab/](https://alternativeto.net/software/gitlab/)\n\nIf you have already had good experiences with other systems, feel free to write them in the comments, I am always open for new technologies.','2018-01-03 00:00:00',0,'host-your-own-vcs');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('PM2 - Advanced Node.js process manager','Operations, Node.JS','You are looking for a simple tutorial to run your Node.js application as a service on Linux? Or do you want to deploy without any downtime? Then the magic word for this is \"pm2\".','Anyone who works with node. js knows how simple it is to build small web applications. But the fact that even complex pages are possible shows us e. g. Airbnb. But that\'s not what this article is about. I\'d rather tell you how to go on - after writing your web application. I had already had the case that my server provider restarted the servers overnight and therefore one of my pages was not accessible. Then I researched what I can do and so I tried dirty hacks with shell scripts to start my Node.js app and put it as a service on Linux. But the answer to my problem was much easier.... It\'s called PM2!\n\n### What is PM2?\n\nPM2 is an advanced, production process manager for Node.js Here is an overview of the advantages it offers you:\n\n-   Behavior configuration\n-   Source map support\n-   Container Integration\n-   Watch & Reload\n-   Log management\n-   Monitoring\n-   Module System\n-   Max memory reload\n-   Cluster Mode\n-   Hot reload\n-   Development workflow\n-   Startup Scripts\n-   Deployment workflow\n-   PaaS Compatible\n-   Keymetrics monitoring\n-   API\n\nI first thought about the functions I use most to describe in more detail. But in my opinion, the documentary is so detailed that I couldn\'t do it better. Therefore, here is the documentation of all functions:  [http://pm2.keymetrics.io/docs/usage/quick-start/](http://pm2.keymetrics.io/docs/usage/quick-start/)\n\nIf you still have any questions, please feel free to comment on this article. I will do my best to answer them!','2018-03-13 00:00:00',0,'nodejs-server-best-practise');
INSERT INTO `whit-e.com`.article (`title`,`tags`,`description`,`content`,`time`,`visible`,`id`) VALUES ('Very simple & fast Node.js Deployment','Operations, Node.JS, PM2','You have your own Linux server? Don\'t want to install tools like Jenkins and Co. there? But still your code should always be up-to-date?\nNo problem. The setup for an automatically deployment (without testing) takes less than 5 minutes!','\nAt the moment I\'m in the process of rebuilding everything here. My private laptop, the blog etc..\n\nSince it always annoyed me that I only updated my blog via FTP (i.e. developed JS files with VS code locally and then shared them via ftp) I thought it would be cool if I quickly build a process to save that ftp way.\nSo I thought about what is the fastest & easiest way to get the decision to build a cronjob that fetches the files from my GitRepo every minute. I also configured PM2 to watch to changes automatically.\n\n## Requirements\n- working [Node.JS App](https://whit-e.com/building-nodejs-rest-api)\n- installed [PM2](https://whit-e.com/nodejs-server-best-practise) \n- GIT-Repository\n\n## Fast Deployment\nFirst we go to our /var/www/ Folder at our SERVER and Checkout the Service: \n```\ngit clone https://github.com/whit-e/microservice_blueprint_nodejs\n```\nInfo: *If Git asks you now for your credentials, we still have to configure that this will not happen in the future. \n\nIt would be the **best way** if we put a ssh key on the root server and link it to our git account: https://help.github.com/en/articles/connecting-to-github-with-ssh\nThe **quickest way** is to execute the command first: `` git config --global credential.helper store``*\n\nNext we go into our project and install all dependencies ``cd yourProject && npm install``\n\nAfter that, we could start our project with **PM2**:\n```pm2 start ./bin/www --watch```\n*The ``--watch `` command ensures that our Node.js application is restarted when file changes are made.*\nIf we now change something at our GitHub Repo and pull it, the changes should be applied automatically.\nIt works? Fine. Now let\'s create our CronJob, which pulls every minute.\n\nFirst we create a new Folder, called **cron** in **/opt/**, after that we switch to this directory and create a simple, executable shell File:\n```shell\nmkdir /opt/cron\ncd /opt/cron\ncat > pullAutomatically.sh\nchmod +x pullAutomatically.sh\n```\nNow we tell our pullAutomatically what exactly should happen:\n``go to the folder of our application`` -> ``do git pull there``\nfor that, edit the file and safe the following there: \n```\n#!/bin/bash\ncd /var/www/yourApplication\ngit pull\n```\n\nEverything what is missing now -> execute this file every minute.\nFor this we use crontab:\n```\ncrontab -e \n#add there\n*/1 * * * * /opt/cron/pullAutomatically.sh\n```\n\nNow we  are finished.\n','2019-10-19 00:00:00',0,'nodejs-easy-deployment');
